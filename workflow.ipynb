{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.6) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
<<<<<<< HEAD
=======
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
>>>>>>> e4a25ad22ba48a591ac0ee02de1e155beffe6c12
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Generate X_train, y_train, X_test, y_test for lag={3, 7} and perc={3perc, 5perc}\n",
    "perc_dict = {'5perc': 0.05, '3perc': 0.03}\n",
    "\n",
<<<<<<< HEAD
    "target_3perc = create_target(0.03)\n",
    "target_3perc = target_3perc.replace({'BUY':-1, 'HOLD':0, 'SELL':1})\n",
    "\n",
    "target_1perc = create_target(0.01)\n",
    "target_1perc = target_1perc.replace({'BUY':-1, 'HOLD':0, 'SELL':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "( 0    1350\n",
       " -1     186\n",
       "  1     143\n",
       " Name: decision, dtype: int64,\n",
       "  0    1573\n",
       "  1      66\n",
       " -1      40\n",
       " Name: decision, dtype: int64,\n",
       "  0    665\n",
       " -1    656\n",
       "  1    358\n",
       " Name: decision, dtype: int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_3perc['decision'].value_counts(), target_5perc['decision'].value_counts(), target_1perc['decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_5perc.copy(deep=True)\n",
    "target = target.drop(['Adj Close'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "## Feature: Index Price ##\n",
    "prices = yf.download(\"^GSPC\", start=\"2015-12-20\", end=\"2022-09-02\")[['Adj Close']]\n",
    "prices = compute_lagged_values(prices, 7, \"mean\")\n",
    "prices = prices.reset_index()\n",
    "prices['Date'] = prices['Date'].apply(lambda x: x.date())\n",
=======
    "for lag in [3, 7]:\n",
    "    for perc in ['5perc', '3perc']:\n",
    "        # target column\n",
    "        target = create_target(perc_dict[perc])\n",
    "        target = target.replace({'BUY':1, 'HOLD':0, 'SELL':-1})\n",
    "        target = target.drop(['Adj Close'], axis=1)\n",
>>>>>>> e4a25ad22ba48a591ac0ee02de1e155beffe6c12
    "\n",
    "        # feature: index price\n",
    "        prices = yf.download(\"^GSPC\", start=\"2015-12-01\", end=\"2022-09-02\")[['Adj Close']]\n",
    "        prices = compute_lagged_values(prices, lag, \"mean\")\n",
    "        prices = prices.reset_index()\n",
    "        prices['Date'] = prices['Date'].apply(lambda x: x.date())\n",
    "        prices = prices.set_index('Date')\n",
    "        prices.index = pd.DatetimeIndex(prices.index)\n",
    "        prices = prices[prices.index.isin(target.index)]\n",
    "\n",
    "        # feature: reddit scores\n",
    "        # WARNING: This line of code may take hours to run. \n",
    "        # reddit_posts = pd.read_excel(\"data/cleaned/reddit_2016_2022_cleaned_1710.xlsx\")\n",
    "        # reddit_scores = reddit_feature_engineer(reddit_posts)\n",
    "        # Instead, run below code to retrieve previously obtained sentiment scores.\n",
    "        reddit_scores = pd.read_excel(\"data/sentiments/reddit_2016_2022_sentiment_scores.xlsx\")\n",
    "        reddit_scores = reddit_scores.set_index('date')\n",
    "        reddit_scores = compute_lagged_values(reddit_scores, lag, \"mean\")\n",
    "        reddit_scores = reddit_scores[reddit_scores.index.isin(target.index)]\n",
    "        weight_type = \"both\" # or \"comments\", \"upvotes\"\n",
    "        reddit_scores = reddit_scores[[f'pos_score_weighted_{weight_type}',f'neg_score_weighted_{weight_type}',f'neu_score_weighted_{weight_type}',f'compound_score_weighted_{weight_type}']]\n",
    "\n",
    "        # feature: news scores\n",
    "        # WARNING: This line of code may take up to an hour to run. \n",
    "        # nyt_posts = pd.read_excel(\"data/cleaned/nyt_2016_2022_cleaned_1710.xlsx\")\n",
    "        # spweights = pd.read_excel(\"data/nyt_2016_2022_cleaned_1710\")\n",
    "        # nyt_scores = nyt_feature_engineer(nyt_posts, spweights)\n",
    "        # nyt_scores = nyt_scores.set_index('date')\n",
    "        # Instead, run below code to retrieve previously obtained sentiment scores.\n",
    "        nyt_scores = pd.read_excel(\"data/sentiments/nyt_2016_2022_sentiment_scores.xlsx\")\n",
    "        nyt_scores = nyt_scores.set_index('date')\n",
    "        nyt_scores = compute_lagged_values(nyt_scores, lag, \"mean\")\n",
    "        nyt_scores = nyt_scores[nyt_scores.index.isin(target.index)]\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "# Then, compute lagged values and filter rows with target value\n",
    "reddit_scores = compute_lagged_values(reddit_scores, 7, \"mean\")\n",
    "reddit_scores = reddit_scores[reddit_scores.index.isin(target.index)]\n",
=======
    "        # feature: macro data\n",
    "        macro_data = pd.read_excel(\"data/raw/Macro_Data_2016_to_2022.xlsx\")\n",
    "        macro_data = macro_feature_engineer(macro_data, data_type=\"both\")\n",
    "        macro_data = macro_data.reindex(target.index)\n",
    "        macro_data = macro_data[macro_data.index.isin(target.index)]\n",
>>>>>>> e4a25ad22ba48a591ac0ee02de1e155beffe6c12
    "\n",
    "        # Combine features and target\n",
    "        data = pd.concat([prices, reddit_scores, nyt_scores, macro_data, target], axis=1)\n",
    "        \n",
    "        # Train-test split\n",
    "        X, y = data.drop(columns={'decision'}), data[['decision']]\n",
    "        X = X.drop(['Unnamed: 0', 'pos_score', 'neg_score','neu_score','compound_score_weighted_both'], axis = 1)\n",
    "        X.rename({'pos_score_weighted_both': 'reddit_pos_both', 'neg_score_weighted_both': 'reddit_neg_both', 'neu_score_weighted_both': 'reddit_neu_both', 'pos_weighted':'nyt_pos','neg_weighted':'nyt_neg','neu_weighted':'nyt_neu','Adj Close':'adj_close','Quarterly GDP (Actual)':'quarterly_gdp_actual','Monthly CPI (Actual)':'monthly_cpi_actual','Monthly Short Term Interest Rates (Actual)':'monthly_st_ir_actual','Monthly Unemployment Rate (Actual)':'monthly_unemployment_actual','Quarterly GDP (Growth)':'quarterly_gdp_growth','Monthly CPI (Growth)':'monthly_cpi_growth','Monthly Short Term Interest Rates (Growth)':'monthly_st_ir_growth','Monthly Unemployment Rate (Growth)':'monthly_unemployment_growth'}, axis = 1, inplace = True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=len(data['2022':]), shuffle=False)\n",
    "\n",
    "        scaler = StandardScaler() \n",
    "        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "        X_train_scaled = X_train_scaled.set_index(X_train.index)\n",
    "\n",
<<<<<<< HEAD
    "# Then, compute lagged values and filter rows with target value\n",
    "nyt_scores = compute_lagged_values(nyt_scores, 7, \"mean\")\n",
    "nyt_scores = nyt_scores[nyt_scores.index.isin(target.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature: Macroeconomic Data ##\n",
    "macro_data = pd.read_excel(\"data/raw/Macro_Data_2016_to_2022.xlsx\")\n",
    "macro_data = macro_feature_engineer(macro_data, normalize=False, data_type=\"actual\") ### TO DELETE NORMALIZE CODE\n",
    "macro_data = macro_data.reindex(target.index)\n",
    "macro_data = macro_data[macro_data.index.isin(target.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and target\n",
    "data = pd.concat([prices, reddit_scores, nyt_scores, macro_data, target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X, y = data.drop(columns={'decision'}), data[['decision']]\n",
    "X = X.drop(['Unnamed: 0', 'pos_score', 'neg_score','neu_score','compound_score_weighted_both'], axis = 1)\n",
    "X.rename({'pos_score_weighted_both': 'reddit_pos_both', 'neg_score_weighted_both': 'reddit_neg_both', 'neu_score_weighted_both': 'reddit_neu_both', 'pos_weighted':'nyt_pos','neg_weighted':'nyt_neg','neu_weighted':'nyt_neu','Adj Close':'adj_close','Quarterly GDP (Actual)':'quarterly_gdp_actual','Monthly CPI (Actual)':'monthly_cpi_actual','Monthly Short Term Interest Rates (Actual)':'monthly_st_ir_actual','Monthly Unemployment Rate (Actual)':'monthly_unemployment_actual'}, axis = 1, inplace = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=len(data['2022':]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train_scaled = X_train_scaled.set_index(X_train.index)\n",
=======
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "        X_test_scaled = X_test_scaled.set_index(X_test.index)\n",
>>>>>>> e4a25ad22ba48a591ac0ee02de1e155beffe6c12
    "\n",
    "        X_train_scaled.to_excel(f'data/base_inputs/x_train_{perc}_lag{lag}.xlsx')\n",
    "        X_test_scaled.to_excel(f'data/base_inputs/x_test_{perc}_lag{lag}.xlsx')\n",
    "        y_train.to_excel(f'data/base_inputs/y_train_{perc}_lag{lag}.xlsx')\n",
    "        y_test.to_excel(f'data/base_inputs/y_test_{perc}_lag{lag}.xlsx')"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1511, 11), (168, 11), (1511, 1), (168, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adj_close                      False\n",
       "reddit_pos_both                False\n",
       "reddit_neg_both                False\n",
       "reddit_neu_both                False\n",
       "nyt_pos                        False\n",
       "nyt_neg                        False\n",
       "nyt_neu                        False\n",
       "quarterly_gdp_actual           False\n",
       "monthly_cpi_actual             False\n",
       "monthly_st_ir_actual           False\n",
       "monthly_unemployment_actual    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_excel('data/model_inputs/x_train_5perc_7lag.xlsx')\n",
    "X_test_scaled.to_excel('data/model_inputs/x_test_5perc_7lag.xlsx')\n",
    "y_train.to_excel('data/model_inputs/y_train_5perc_7lag.xlsx')\n",
    "y_test.to_excel('data/model_inputs/y_test_5perc_7lag.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> e4a25ad22ba48a591ac0ee02de1e155beffe6c12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d4736924dd4ee01619834d3df5aac36876c141d93beb7feebfa3d7eb88b873d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
