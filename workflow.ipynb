{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "## Target column ##\n",
    "target_5perc = create_target(0.05)\n",
    "target_5perc = target_5perc.replace({'BUY':-1, 'HOLD':0, 'SELL':1})\n",
    "\n",
    "target_3perc = create_target(0.03)\n",
    "target_3perc = target_3perc.replace({'BUY':-1, 'HOLD':0, 'SELL':1})\n",
    "\n",
    "target_1perc = create_target(0.01)\n",
    "target_1perc = target_1perc.replace({'BUY':-1, 'HOLD':0, 'SELL':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "( 0    1350\n",
       " -1     186\n",
       "  1     143\n",
       " Name: decision, dtype: int64,\n",
       "  0    1573\n",
       "  1      66\n",
       " -1      40\n",
       " Name: decision, dtype: int64,\n",
       "  0    665\n",
       " -1    656\n",
       "  1    358\n",
       " Name: decision, dtype: int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_3perc['decision'].value_counts(), target_5perc['decision'].value_counts(), target_1perc['decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_5perc.copy(deep=True)\n",
    "target = target.drop(['Adj Close'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "## Feature: Index Price ##\n",
    "prices = yf.download(\"^GSPC\", start=\"2015-12-20\", end=\"2022-09-02\")[['Adj Close']]\n",
    "prices = compute_lagged_values(prices, 7, \"mean\")\n",
    "prices = prices.reset_index()\n",
    "prices['Date'] = prices['Date'].apply(lambda x: x.date())\n",
    "\n",
    "prices = prices.set_index('Date')\n",
    "prices.index = pd.DatetimeIndex(prices.index)\n",
    "\n",
    "prices = prices[prices.index.isin(target.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature: Reddit Posts ##\n",
    "# WARNING: This line of code may take up to an hour to run. \n",
    "# reddit_posts = pd.read_excel(\"data/cleaned/reddit_2016_2022_cleaned_1710.xlsx\")\n",
    "# reddit_scores = reddit_feature_engineer(reddit_posts)\n",
    "\n",
    "# Instead, run this to directly retrieve reddit sentiment scores.\n",
    "reddit_scores = pd.read_excel(\"data/sentiments/reddit_2016_2022_sentiment_scores.xlsx\")\n",
    "reddit_scores = reddit_scores.set_index('date')\n",
    "\n",
    "# Then, compute lagged values and filter rows with target value\n",
    "reddit_scores = compute_lagged_values(reddit_scores, 7, \"mean\")\n",
    "reddit_scores = reddit_scores[reddit_scores.index.isin(target.index)]\n",
    "\n",
    "weight_type = \"both\" # or \"comments\", \"upvotes\"\n",
    "reddit_scores = reddit_scores[[f'pos_score_weighted_{weight_type}',f'neg_score_weighted_{weight_type}',f'neu_score_weighted_{weight_type}',f'compound_score_weighted_{weight_type}']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature: NYT Articles ###\n",
    "# WARNING: This line of code may take up to an hour to run. \n",
    "# nyt_posts = pd.read_excel(\"data/cleaned/nyt_2016_2022_cleaned_1710.xlsx\")\n",
    "# spweights = pd.read_excel(\"data/nyt_2016_2022_cleaned_1710\")\n",
    "# nyt_scores = nyt_feature_engineer(nyt_posts, spweights)\n",
    "# nyt_scores = nyt_scores.set_index('date')\n",
    "\n",
    "# Instead, run this to directly retrieve nyt sentiment scores.\n",
    "nyt_scores = pd.read_excel(\"data/sentiments/nyt_2016_2022_sentiment_scores.xlsx\")\n",
    "nyt_scores = nyt_scores.set_index('date')\n",
    "\n",
    "# Then, compute lagged values and filter rows with target value\n",
    "nyt_scores = compute_lagged_values(nyt_scores, 7, \"mean\")\n",
    "nyt_scores = nyt_scores[nyt_scores.index.isin(target.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature: Macroeconomic Data ##\n",
    "macro_data = pd.read_excel(\"data/raw/Macro_Data_2016_to_2022.xlsx\")\n",
    "macro_data = macro_feature_engineer(macro_data, normalize=False, data_type=\"actual\") ### TO DELETE NORMALIZE CODE\n",
    "macro_data = macro_data.reindex(target.index)\n",
    "macro_data = macro_data[macro_data.index.isin(target.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and target\n",
    "data = pd.concat([prices, reddit_scores, nyt_scores, macro_data, target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X, y = data.drop(columns={'decision'}), data[['decision']]\n",
    "X = X.drop(['Unnamed: 0', 'pos_score', 'neg_score','neu_score','compound_score_weighted_both'], axis = 1)\n",
    "X.rename({'pos_score_weighted_both': 'reddit_pos_both', 'neg_score_weighted_both': 'reddit_neg_both', 'neu_score_weighted_both': 'reddit_neu_both', 'pos_weighted':'nyt_pos','neg_weighted':'nyt_neg','neu_weighted':'nyt_neu','Adj Close':'adj_close','Quarterly GDP (Actual)':'quarterly_gdp_actual','Monthly CPI (Actual)':'monthly_cpi_actual','Monthly Short Term Interest Rates (Actual)':'monthly_st_ir_actual','Monthly Unemployment Rate (Actual)':'monthly_unemployment_actual'}, axis = 1, inplace = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=len(data['2022':]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train_scaled = X_train_scaled.set_index(X_train.index)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test_scaled = X_test_scaled.set_index(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1511, 11), (168, 11), (1511, 1), (168, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adj_close                      False\n",
       "reddit_pos_both                False\n",
       "reddit_neg_both                False\n",
       "reddit_neu_both                False\n",
       "nyt_pos                        False\n",
       "nyt_neg                        False\n",
       "nyt_neu                        False\n",
       "quarterly_gdp_actual           False\n",
       "monthly_cpi_actual             False\n",
       "monthly_st_ir_actual           False\n",
       "monthly_unemployment_actual    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_excel('data/model_inputs/x_train_5perc_7lag.xlsx')\n",
    "X_test_scaled.to_excel('data/model_inputs/x_test_5perc_7lag.xlsx')\n",
    "y_train.to_excel('data/model_inputs/y_train_5perc_7lag.xlsx')\n",
    "y_test.to_excel('data/model_inputs/y_test_5perc_7lag.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d4736924dd4ee01619834d3df5aac36876c141d93beb7feebfa3d7eb88b873d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
