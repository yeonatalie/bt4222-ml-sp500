{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.6) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Generate X_train, y_train, X_test, y_test for lag={3, 7} and perc={3perc, 5perc}\n",
    "perc_dict = {'5perc': 0.05, '3perc': 0.03}\n",
    "\n",
    "for lag in [3, 7]:\n",
    "    for perc in ['5perc', '3perc']:\n",
    "        # target column\n",
    "        target = create_target(perc_dict[perc])\n",
    "        target = target.replace({'BUY':1, 'HOLD':0, 'SELL':-1})\n",
    "        target = target.drop(['Adj Close'], axis=1)\n",
    "\n",
    "        # feature: index price\n",
    "        prices = yf.download(\"^GSPC\", start=\"2015-12-01\", end=\"2022-09-02\")[['Adj Close']]\n",
    "        prices = compute_lagged_values(prices, lag, \"mean\")\n",
    "        prices = prices.reset_index()\n",
    "        prices['Date'] = prices['Date'].apply(lambda x: x.date())\n",
    "        prices = prices.set_index('Date')\n",
    "        prices.index = pd.DatetimeIndex(prices.index)\n",
    "        prices = prices[prices.index.isin(target.index)]\n",
    "\n",
    "        # feature: reddit scores\n",
    "        # WARNING: This line of code may take hours to run. \n",
    "        # reddit_posts = pd.read_excel(\"data/cleaned/reddit_2016_2022_cleaned_1710.xlsx\")\n",
    "        # reddit_scores = reddit_feature_engineer(reddit_posts)\n",
    "        # Instead, run below code to retrieve previously obtained sentiment scores.\n",
    "        reddit_scores = pd.read_excel(\"data/sentiments/reddit_2016_2022_sentiment_scores.xlsx\")\n",
    "        reddit_scores = reddit_scores.set_index('date')\n",
    "        reddit_scores = compute_lagged_values(reddit_scores, lag, \"mean\")\n",
    "        reddit_scores = reddit_scores[reddit_scores.index.isin(target.index)]\n",
    "        weight_type = \"both\" # or \"comments\", \"upvotes\"\n",
    "        reddit_scores = reddit_scores[[f'pos_score_weighted_{weight_type}',f'neg_score_weighted_{weight_type}',f'neu_score_weighted_{weight_type}',f'compound_score_weighted_{weight_type}']]\n",
    "\n",
    "        # feature: news scores\n",
    "        # WARNING: This line of code may take up to an hour to run. \n",
    "        # nyt_posts = pd.read_excel(\"data/cleaned/nyt_2016_2022_cleaned_1710.xlsx\")\n",
    "        # spweights = pd.read_excel(\"data/nyt_2016_2022_cleaned_1710\")\n",
    "        # nyt_scores = nyt_feature_engineer(nyt_posts, spweights)\n",
    "        # nyt_scores = nyt_scores.set_index('date')\n",
    "        # Instead, run below code to retrieve previously obtained sentiment scores.\n",
    "        nyt_scores = pd.read_excel(\"data/sentiments/nyt_2016_2022_sentiment_scores.xlsx\")\n",
    "        nyt_scores = nyt_scores.set_index('date')\n",
    "        nyt_scores = compute_lagged_values(nyt_scores, lag, \"mean\")\n",
    "        nyt_scores = nyt_scores[nyt_scores.index.isin(target.index)]\n",
    "\n",
    "        # feature: macro data\n",
    "        macro_data = pd.read_excel(\"data/raw/Macro_Data_2016_to_2022.xlsx\")\n",
    "        macro_data = macro_feature_engineer(macro_data, data_type=\"both\")\n",
    "        macro_data = macro_data.reindex(target.index)\n",
    "        macro_data = macro_data[macro_data.index.isin(target.index)]\n",
    "\n",
    "        # Combine features and target\n",
    "        data = pd.concat([prices, reddit_scores, nyt_scores, macro_data, target], axis=1)\n",
    "        \n",
    "        # Train-test split\n",
    "        X, y = data.drop(columns={'decision'}), data[['decision']]\n",
    "        X = X.drop(['Unnamed: 0', 'pos_score', 'neg_score','neu_score','compound_score_weighted_both'], axis = 1)\n",
    "        X.rename({'pos_score_weighted_both': 'reddit_pos_both', 'neg_score_weighted_both': 'reddit_neg_both', 'neu_score_weighted_both': 'reddit_neu_both', 'pos_weighted':'nyt_pos','neg_weighted':'nyt_neg','neu_weighted':'nyt_neu','Adj Close':'adj_close','Quarterly GDP (Actual)':'quarterly_gdp_actual','Monthly CPI (Actual)':'monthly_cpi_actual','Monthly Short Term Interest Rates (Actual)':'monthly_st_ir_actual','Monthly Unemployment Rate (Actual)':'monthly_unemployment_actual','Quarterly GDP (Growth)':'quarterly_gdp_growth','Monthly CPI (Growth)':'monthly_cpi_growth','Monthly Short Term Interest Rates (Growth)':'monthly_st_ir_growth','Monthly Unemployment Rate (Growth)':'monthly_unemployment_growth'}, axis = 1, inplace = True)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=len(data['2022':]), shuffle=False)\n",
    "\n",
    "        scaler = StandardScaler() \n",
    "        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "        X_train_scaled = X_train_scaled.set_index(X_train.index)\n",
    "\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "        X_test_scaled = X_test_scaled.set_index(X_test.index)\n",
    "\n",
    "        X_train_scaled.to_excel(f'data/model_inputs/x_train_{perc}_lag{lag}.xlsx')\n",
    "        X_test_scaled.to_excel(f'data/model_inputs/x_test_{perc}_lag{lag}.xlsx')\n",
    "        y_train.to_excel(f'data/model_inputs/y_train_{perc}_lag{lag}.xlsx')\n",
    "        y_test.to_excel(f'data/model_inputs/y_test_{perc}_lag{lag}.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
