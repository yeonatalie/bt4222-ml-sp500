{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(filename, start_col, end_col, clean_usa=False, deemojize=False, clean_punctuation=False, remove_stopwords=False, lemmatize=False, stemming=False):\n",
    "    df = pd.read_excel(filename)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_text_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # concatenate text columns\n",
    "        text = ' '.join(row[start_col:end_col+1].dropna().astype(str)) \n",
    "        \n",
    "        # lower case\n",
    "        text = text.lower()\n",
    "\n",
    "        if 'reddit' in filename:\n",
    "            text = '' if (str(text) == 'nan' or str(text) == '[removed]') else str(text)\n",
    "            text = text.replace('amp;', '')\n",
    "\n",
    "        if clean_usa:\n",
    "            text = text.replace('u.s.', 'united states').replace('u.s.a.', 'united states')\n",
    "    \n",
    "        if deemojize:\n",
    "            text = emoji.demojize(text)\n",
    "    \n",
    "        if clean_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        if remove_stopwords:\n",
    "            tokens = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "        if lemmatize:\n",
    "            # POS tagging\n",
    "            tokens = [nltk.pos_tag([word]) for word in tokens]\n",
    "            # lemmatization\n",
    "            tokens = [lemmatizer.lemmatize(word[0][0], get_wordnet_pos(word[0][1])) \n",
    "                if get_wordnet_pos(word[0][1])!=None else lemmatizer.lemmatize(word[0][0]) for word in tokens]\n",
    "    \n",
    "        if stemming:\n",
    "            tokens = [ps.stem(word) for word in tokens]\n",
    "    \n",
    "        # concatenate tokens back\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "        if idx % 1000 == 0:\n",
    "            print(f'---- {idx} DONE ----')\n",
    "\n",
    "    df['cleaned_text'] = cleaned_text_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 DONE ----\n",
      "---- 1000 DONE ----\n",
      "---- 2000 DONE ----\n",
      "---- 3000 DONE ----\n",
      "---- 4000 DONE ----\n",
      "---- 5000 DONE ----\n",
      "---- 6000 DONE ----\n",
      "---- 7000 DONE ----\n",
      "---- 8000 DONE ----\n",
      "---- 9000 DONE ----\n",
      "---- 10000 DONE ----\n",
      "---- 11000 DONE ----\n",
      "---- 12000 DONE ----\n",
      "---- 13000 DONE ----\n",
      "---- 14000 DONE ----\n",
      "---- 15000 DONE ----\n",
      "---- 16000 DONE ----\n",
      "---- 17000 DONE ----\n",
      "---- 18000 DONE ----\n"
     ]
    }
   ],
   "source": [
    "nyt_df = clean_text('../data/raw/nyt_2016_2022_final.xlsx', start_col=2, end_col=4, clean_usa=True, deemojize=True, clean_punctuation=True, remove_stopwords=True, lemmatize=True, stemming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>keyword</th>\n",
       "      <th>headline</th>\n",
       "      <th>abstract</th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>section</th>\n",
       "      <th>hits</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>['Microsoft', 'Tesla']</td>\n",
       "      <td>Looking Beyond the Internet of Things</td>\n",
       "      <td>Adam Bosworth, a tech pioneer, sees the future...</td>\n",
       "      <td>SAN FRANCISCO — If you have sent email on Goog...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>1259</td>\n",
       "      <td>1180</td>\n",
       "      <td>look beyond internet thing adam bosworth tech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>[\"McDonald's\"]</td>\n",
       "      <td>No More Statutes of Limitations for Rape</td>\n",
       "      <td>Bill Cosby came close to escaping sexual assau...</td>\n",
       "      <td>THIS week, Bill Cosby was charged with three c...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>1514</td>\n",
       "      <td>914</td>\n",
       "      <td>statute limitation rape bill cosby come close ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>['Visa']</td>\n",
       "      <td>U.S. Doesn’t Know How Many Foreign Visitors Ov...</td>\n",
       "      <td>After two decades of failed attempts to track ...</td>\n",
       "      <td>WASHINGTON — The question from the congressman...</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>2394</td>\n",
       "      <td>1189</td>\n",
       "      <td>united state doesnt know many foreign visitor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>['Fed']</td>\n",
       "      <td>Making And Using Models</td>\n",
       "      <td>It’s about self-discipline.</td>\n",
       "      <td>Larry Summers, Brad DeLong, and yours truly ar...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>3998</td>\n",
       "      <td>576</td>\n",
       "      <td>make use model selfdiscipline larry summer bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>['Amazon']</td>\n",
       "      <td>Cutting the Cord and Feeling Good About It</td>\n",
       "      <td>Canceling cable has meant becoming more intent...</td>\n",
       "      <td>Nearly three years ago, when I first thought a...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>6215</td>\n",
       "      <td>440</td>\n",
       "      <td>cut cord feel good cancel cable meant become i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                 keyword  \\\n",
       "0 2016-01-01  ['Microsoft', 'Tesla']   \n",
       "1 2016-01-01          [\"McDonald's\"]   \n",
       "2 2016-01-01                ['Visa']   \n",
       "3 2016-01-02                 ['Fed']   \n",
       "4 2016-01-02              ['Amazon']   \n",
       "\n",
       "                                            headline  \\\n",
       "0              Looking Beyond the Internet of Things   \n",
       "1           No More Statutes of Limitations for Rape   \n",
       "2  U.S. Doesn’t Know How Many Foreign Visitors Ov...   \n",
       "3                            Making And Using Models   \n",
       "4         Cutting the Cord and Feeling Good About It   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Adam Bosworth, a tech pioneer, sees the future...   \n",
       "1  Bill Cosby came close to escaping sexual assau...   \n",
       "2  After two decades of failed attempts to track ...   \n",
       "3                        It’s about self-discipline.   \n",
       "4  Canceling cable has meant becoming more intent...   \n",
       "\n",
       "                                      lead_paragraph     section  hits  \\\n",
       "0  SAN FRANCISCO — If you have sent email on Goog...  Technology  1259   \n",
       "1  THIS week, Bill Cosby was charged with three c...     Opinion  1514   \n",
       "2  WASHINGTON — The question from the congressman...        U.S.  2394   \n",
       "3  Larry Summers, Brad DeLong, and yours truly ar...     Opinion  3998   \n",
       "4  Nearly three years ago, when I first thought a...     Opinion  6215   \n",
       "\n",
       "   word_count                                       cleaned_text  \n",
       "0        1180  look beyond internet thing adam bosworth tech ...  \n",
       "1         914  statute limitation rape bill cosby come close ...  \n",
       "2        1189  united state doesnt know many foreign visitor ...  \n",
       "3         576  make use model selfdiscipline larry summer bra...  \n",
       "4         440  cut cord feel good cancel cable meant become i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_df.to_excel('../data/cleaned/nyt_2016_2022_cleaned_1710.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 DONE ----\n",
      "---- 1000 DONE ----\n",
      "---- 2000 DONE ----\n",
      "---- 3000 DONE ----\n",
      "---- 4000 DONE ----\n",
      "---- 5000 DONE ----\n",
      "---- 6000 DONE ----\n",
      "---- 7000 DONE ----\n",
      "---- 8000 DONE ----\n",
      "---- 9000 DONE ----\n",
      "---- 10000 DONE ----\n",
      "---- 11000 DONE ----\n",
      "---- 12000 DONE ----\n",
      "---- 13000 DONE ----\n",
      "---- 14000 DONE ----\n",
      "---- 15000 DONE ----\n",
      "---- 16000 DONE ----\n",
      "---- 17000 DONE ----\n",
      "---- 18000 DONE ----\n",
      "---- 19000 DONE ----\n",
      "---- 20000 DONE ----\n",
      "---- 21000 DONE ----\n",
      "---- 22000 DONE ----\n",
      "---- 23000 DONE ----\n",
      "---- 24000 DONE ----\n",
      "---- 25000 DONE ----\n",
      "---- 26000 DONE ----\n",
      "---- 27000 DONE ----\n",
      "---- 28000 DONE ----\n",
      "---- 29000 DONE ----\n",
      "---- 30000 DONE ----\n",
      "---- 31000 DONE ----\n",
      "---- 32000 DONE ----\n",
      "---- 33000 DONE ----\n",
      "---- 34000 DONE ----\n",
      "---- 35000 DONE ----\n",
      "---- 36000 DONE ----\n",
      "---- 37000 DONE ----\n",
      "---- 38000 DONE ----\n",
      "---- 39000 DONE ----\n",
      "---- 40000 DONE ----\n",
      "---- 41000 DONE ----\n",
      "---- 42000 DONE ----\n",
      "---- 43000 DONE ----\n",
      "---- 44000 DONE ----\n",
      "---- 45000 DONE ----\n",
      "---- 46000 DONE ----\n",
      "---- 47000 DONE ----\n",
      "---- 48000 DONE ----\n",
      "---- 49000 DONE ----\n",
      "---- 50000 DONE ----\n",
      "---- 51000 DONE ----\n",
      "---- 52000 DONE ----\n",
      "---- 53000 DONE ----\n",
      "---- 54000 DONE ----\n",
      "---- 55000 DONE ----\n",
      "---- 56000 DONE ----\n",
      "---- 57000 DONE ----\n",
      "---- 58000 DONE ----\n",
      "---- 59000 DONE ----\n",
      "---- 60000 DONE ----\n",
      "---- 61000 DONE ----\n",
      "---- 62000 DONE ----\n",
      "---- 63000 DONE ----\n",
      "---- 64000 DONE ----\n",
      "---- 65000 DONE ----\n",
      "---- 66000 DONE ----\n",
      "---- 67000 DONE ----\n",
      "---- 68000 DONE ----\n",
      "---- 69000 DONE ----\n",
      "---- 70000 DONE ----\n",
      "---- 71000 DONE ----\n",
      "---- 72000 DONE ----\n",
      "---- 73000 DONE ----\n",
      "---- 74000 DONE ----\n",
      "---- 75000 DONE ----\n"
     ]
    }
   ],
   "source": [
    "reddit_df = clean_text('../data/raw/reddit_2016_2022_final.xlsx', start_col=5, end_col=6, clean_usa=True, deemojize=True, clean_punctuation=True, remove_stopwords=True, lemmatize=True, stemming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.to_excel('../data/cleaned/reddit_2016_2022_cleaned_1710.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d4736924dd4ee01619834d3df5aac36876c141d93beb7feebfa3d7eb88b873d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
